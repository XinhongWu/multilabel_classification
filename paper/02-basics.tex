\section{Methoden}
\subsection{Sampling}

Zunächst haben wir den Datensatz in Test- und Trainingsdaten im Verhältnis 3:7 aufgeteilt, um die gelernten Modelle auf ungesehen Daten testen zu können.
Für die Aufteilung wurde \emph{stratified sampling} verwendet, sodass die Klassen anteilig in Test- und Trainingsdaten vertreten sind.
Wichtig hierbei ist es, dass wir anstatt der einzelnen Klassen die vergebenen Klassenkombinationen verwenden, sodass die Kombinationen von Klassen anteilig in den beiden Datensätzen auftreten.

\subsection{Multilabel Klassifikation}
\label{sub:multilabel_klassifikation}
Es existieren zwei Arten von Ansätzen, um das Multilabel Klassifikation Problem zu lösen.

Um mit gängigen Verfahren zur Klassifikation das Problem zu lösen werden die Labels transformiert,
so dass die Labels von bekannten Algorithmen verwendet werden können. Wir verwenden die Transformation der Labelkombinationen,
dass heißt jede Kombination der Labels wird als eine Klasse angesehen, wobei die Labels sortiert sind. Also werden die Vergaben der Labels
$l_1,l,2$ und $l_2,l_1$ als gleich angesehen.

Die andere Möglichkeit besteht darin bestehende Verfahren und Algorithmen auf das Mutlilabel Klassifikationproblem zu adaptieren.
Wir verwenden hierfür einen sogenannten One-Vs-Rest-Klassifikator, der für jede Klasse $c_i$ einen binären Klassifikator lernt.
Weitere Möglichkeiten um das Problem der Multilabelklassifikation anzugehen sind in \cite{Tsoumakas07multi-labelclassification:} zu finden.

\subsection{Support Vector Machines}
\label{sub:support_vector_machines}
Die \emph{Support Vector Machine} (SVM) ist ein Verfahren des überwachten Lernens, das zum Klassifizieren von Objekten verwendet wird.
Zum Lernen einer SVM werden die Trainingsdaten als Vektoren, zusammen mit den zugehörigen Klassen übergeben.
Für Texte ist die Repräsentation als Vektor meistens das Vektorspace-Modell, bei dem jedes Wort in dem Text als Feature angesehen wird.
Die SVM versucht dann anhand dieser Daten eine Hyperebene in den Raum zu legen, so dass die Trainingsdaten in zwei Klassen eingeteilt werden.
Der Abstand der Vektoren, die der Hyperebene am nächsten liegen, wird dabei maximiert.
Ursprünglich wurden SVMs zur Unterteilung der Objekte in nur zwei Klassen konzipiert.
Es wurde gezeigt, dass Support Vector Machines eines der besten Verfahren zur Klassifikation sind \cite{Joachims:1998:TCS:645326.649721}.

Um Support Vector Machines für das Multilabel Klassifizierungsproblem zu adaptieren, wird für jede Klasse $c \in L$ ein $\tilde c$ bestimmt, sodass für ein Dokument $D_i$ mit den Labels $L_i$ gilt:
\[
    \tilde c =
    \begin{cases}
        1 &\mbox{wenn } c \in L_i \\
        0 &\mbox{sonst}
    \end{cases}
\]

wobei $\tilde c$ die neue zu lernende Klasse für den binären Klassifkator repräsentiert.
Zur Klassifikation von neuen ungesehen Dokumenten wird nun jeder der binären Klassifikatoren gefragt, ob das aktuelle Dokument zu der gelernten Klasse gehört.
Ist die Antwort positiv, wird sich für dieses Dokument diese Klasse notiert, so dass man am Ende eine Menge von Klassen für das Dokument erhält.


\subsection{Latent Dirichlet Allocation}
\label{sub:latent_dirichlet_allocation}

Die \emph{Latent Dirichlet Allocation} (LDA) ist ein generatives stochastisches Modell und wurde in \cite{Blei:2003:LDA:944919.944937} eingeführt.
LDA lernt die in einer Textkollektion zugrundeliegenen Themen(Topics) der Dokumente. Die Anzahl der Topics muss vorher bekannt sein.
Die Topics sind Dirichletverteilungen über Wörtern und die Dokumente werden wiederum als eine Dirichletverteilung über Topics modelliert.
Über diese Verteilungen lassen sich Ähnlichkeiten der Dokumente ableiten, da sie ähnliche zugrundeliegende Themen diskutieren.

Die Verteilung über die Topics verwenden wir als neue Merkmale für die Dokumenten, um sie mit einer Support Vektor Machine zu klassifizieren.
Da dadurch die Anzahl der Features sehr viel kleiner wird bei uns max. 190 Merkmale, kann man von einer Art Featurereduktion sprechen.

\subsection{Evaluierungsmaße}
Um die erzielten Ergebnisse auswerten und mit einander vergleichen zu können, haben wir die gängigen Maße \emph{Precision}, \emph{Recall} und \emph{F1} genutzt.
Vollständigkeitshalber werden hier nochmal die entsprechenden Gleichungen aufgeführt.

Precision \[\frac{\#(\text{relevant items retrieved})}{\#(\text{retrieved items})}\]
Recall \[\frac{\#(\text{relevant items retrieved})}{\#(\text{relevant items})}\]
F1-Measure \[F_1 = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}\]

Ferner wurden zwei verschiedene Ansätze genutzt, um die Ergebnisse für ein Verfahren über alle Klassen zusammenzufassen.
Es wurden über alle Klassen sowohl die Micro- als auch die Macro-Precision, Recall und F1-Measure berechnet.
Die Macro-Average-Maße berechnen einen einfachen Durchschnitt über alle Klassen, während die Micro-Average-Maße die Klassen nach deren Größen gewichten \cite{Manning:2008:IIR:1394399}.

% TODO: micro/macro average F1-Measure --> nachdem P. die Ergebnisse korrigiert hat
%"Macroaveraging computes a simple average over classes.....
%The differences between the two methods can be large. Macroaveraging
%gives equal weight to each class, whereas microaveraging gives equal weight
%to each per-document classification decision."
